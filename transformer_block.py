from multi_head_attention import MultiHeadAttention


class TransformerBlock:
    def __init__(self, embed_dim, num_heads, ):
        MultiHeadAttention